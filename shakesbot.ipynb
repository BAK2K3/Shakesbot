{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Model/layer imports\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #If using GPU, reset memory allocation, and assign GPU with memory growth.\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Shakespeare.txt\n",
    "path_to_file = \"shakesbot.txt\"\n",
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain unique vocab set and size \n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index mapping for characters in vocab (bi-directional)\n",
    "char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode whole of text file using vocab index map\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Query line(s) length(s)\n",
    "# line = 'From fairest creatures we desire increase'\n",
    "# print(len(line))\n",
    "\n",
    "# lines='''\n",
    "#   From fairest creatures we desire increase,\n",
    "#   That thereby beauty's rose might never die,\n",
    "#   But as the riper should by time decease,\n",
    "#   His tender heir might bear his memory:\n",
    "#   '''\n",
    "# print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set length of sequence\n",
    "seq_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine total number of sequences in text\n",
    "total_num_seq = len(text) // (seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Character Dataset from encoded text \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine into sequences\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for splitting sequences into input/target \n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "#Use method to create complete training dataset\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "buffer_size=10000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set embedding dimensions\n",
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose amount of neurons in LSTM layer\n",
    "rnn_neurons = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customised Sparse_cat_cross loss function \n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating model\n",
    "def create_model(vocab_size, embed_dim,rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size,None]))\n",
    "    model.add(LSTM(rnn_neurons, stateful=True, return_sequences=True, recurrent_initializer='glorot_uniform', batch_input_shape=[batch_size,None, embed_dim]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile('adam', loss=sparse_cat_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training model\n",
    "model = create_model(vocab_size=vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set epochs \n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create early stopping function\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 2.0706\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 1.4518\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 1.3117\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.2466\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.2047\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.1748\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.1512\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 36s 65ms/step - loss: 1.1312\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.1137\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 36s 64ms/step - loss: 1.0987\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 37s 66ms/step - loss: 1.0844\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 36s 64ms/step - loss: 1.0714\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.0588\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 1.0479\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 1.0377\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.0283\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.0198\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.0110\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 1.0041\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9979\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9920\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9867\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9832\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9794\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9770\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9734\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 0.9778\n",
      "Epoch 28/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9689\n",
      "Epoch 29/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9674\n",
      "Epoch 30/100\n",
      "563/563 [==============================] - 35s 62ms/step - loss: 0.9649\n",
      "Epoch 31/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9682\n",
      "Epoch 32/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9665\n",
      "Epoch 33/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9645\n",
      "Epoch 34/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9627\n",
      "Epoch 35/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9648\n",
      "Epoch 36/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9652\n",
      "Epoch 37/100\n",
      "563/563 [==============================] - 35s 63ms/step - loss: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc23cfba90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(dataset,epochs=epochs,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('shakesbot_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new version of the model (test_model) with a single input batch size\n",
    "test_model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)\n",
    "test_model.load_weights('shakesbot_train.h5')\n",
    "test_model.build(tf.TensorShape([1,None]))\n",
    "test_model.save('shakesbot_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            21504     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display test model summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE BELOW CODE IF YOU ARE OPENING THIS AFTER TRAINING, AND ALREADY HAVE A PRE-TRAINED .h5 FILE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If loading seperately\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import numpy as np\n",
    "\n",
    "# # #if using GPU, set the correct GPU and assign memory growth\n",
    "# # tf.compat.v1.reset_default_graph()\n",
    "# # physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# # tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "# #Open shakesbot.txt\n",
    "# path_to_file = \"shakesbot.txt\"\n",
    "# text = open(path_to_file, 'r').read()\n",
    "\n",
    "# #Obtain unique vocab set and size \n",
    "# vocab = sorted(set(text))\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# #Create an index mapping for characters in vocab (bi-directional)\n",
    "# char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "# ind_to_char = np.array(vocab)\n",
    "\n",
    "# #Load the model to memory\n",
    "# test_model = load_model('shakesbot_test.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE ABOVE CODE IF YOU ARE OPENING THIS AFTER TRAINING, AND ALREADY HAVE A PRE-TRAINED .h5 FILE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test function for generating consecutive text\n",
    "def generate_text(model,start_seed,num_generate=500,temperature=1):\n",
    "    \n",
    "    #Map each character in start_seed to it's relative index\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    \n",
    "    #Expand the dimensions\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    #Create empty list for generated text\n",
    "    text_generated = []\n",
    "    \n",
    "    #Reset the states of tyhe model        \n",
    "    model.reset_states()\n",
    "    \n",
    "    #for each iteration of num_generate\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        #Obtain probability matrix for current iteration \n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        #Reduce dimensions\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        #Multiply probabily matrix by temperature\n",
    "        predictions = predictions/temperature\n",
    "        \n",
    "        #Select a random outcome, based on the unnormalised log-probabilities produced by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #Expand dimensions of prediction and assign as next input evaluation\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        #Convert prediction to char and append to generated list of text\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        \n",
    "    #return the initial input, concatenated with the generated text. \n",
    "    return(start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a testimony that the world should love his own precious breath.\n",
      "    The husband that hath struck the same for                                                        Exit\n",
      "  FLUTE. There is no worse a kind of princely father\n",
      "    That should revenge the heavy time of thine.\n",
      "    Then when thou speak'st not not which thou wilt fear\n",
      "    The flattering of thy life I should suffer\n",
      "    A present that I thought he shall be sold.\n",
      "    Then, to thy spirit, dear to thee, are here.\n",
      "                                  \n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample text\n",
    "print(generate_text(model=test_model,start_seed=\"This is a test\", num_generate=500, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
