{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Model/layer imports\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #If using GPU, reset memory allocation, and assign GPU with memory growth.\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Shakespeare.txt\n",
    "path_to_file = \"shakesbot.txt\"\n",
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain unique vocab set and size \n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index mapping for characters in vocab (bi-directional)\n",
    "char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode whole of text file using vocab index map\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Query line(s) length(s)\n",
    "# line = 'From fairest creatures we desire increase'\n",
    "# print(len(line))\n",
    "\n",
    "# lines='''\n",
    "#   From fairest creatures we desire increase,\n",
    "#   That thereby beauty's rose might never die,\n",
    "#   But as the riper should by time decease,\n",
    "#   His tender heir might bear his memory:\n",
    "#   '''\n",
    "# print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set length of sequence\n",
    "seq_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine total number of sequences in text\n",
    "total_num_seq = len(text) // (seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Character Dataset from encoded text \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine into sequences\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for splitting sequences into input/target \n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "#Use method to create complete training dataset\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "buffer_size=10000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set embedding dimensions\n",
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose amount of neurons in LSTM layer\n",
    "rnn_neurons = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customised Sparse_cat_cross loss function \n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating model\n",
    "def create_model(vocab_size, embed_dim,rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size,None]))\n",
    "    model.add(LSTM(rnn_neurons, stateful=True, return_sequences=True, recurrent_initializer='glorot_uniform', batch_input_shape=[batch_size,None, embed_dim]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile('adam', loss=sparse_cat_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training model\n",
    "model = create_model(vocab_size=vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set epochs \n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create early stopping function\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "model.fit(dataset,epochs=epochs,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('shakesbot_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new version of the model (test_model) with a single input batch size\n",
    "test_model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)\n",
    "test_model.load_weights('shakesbot_train.h5')\n",
    "test_model.build(tf.TensorShape([1,None]))\n",
    "test_model.save('shakesbot_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test model summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE BELOW CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If loading seperately\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import numpy as np\n",
    "\n",
    "# # #if using GPU, set the correct GPU and assign memory growth\n",
    "# # tf.compat.v1.reset_default_graph()\n",
    "# # physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# # tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "# #Open shakesbot.txt\n",
    "# path_to_file = \"shakesbot.txt\"\n",
    "# text = open(path_to_file, 'r').read()\n",
    "\n",
    "# #Obtain unique vocab set and size \n",
    "# vocab = sorted(set(text))\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# #Create an index mapping for characters in vocab (bi-directional)\n",
    "# char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "# ind_to_char = np.array(vocab)\n",
    "\n",
    "# #Load the model to memory\n",
    "# test_model = load_model('shakesbot_test.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE ABOVE CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test function for generating consecutive text\n",
    "def generate_text(model,start_seed,num_generate=500,temperature=1):\n",
    "    \n",
    "    #Map each character in start_seed to it's relative index\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    \n",
    "    #Expand the dimensions\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    #Create empty list for generated text\n",
    "    text_generated = []\n",
    "    \n",
    "    #Reset the states of tyhe model        \n",
    "    model.reset_states()\n",
    "    \n",
    "    #for each iteration of num_generate\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        #Obtain probability matrix for current iteration \n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        #Reduce dimensions\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        #Multiply probabily matrix by temperature\n",
    "        predictions = predictions/temperature\n",
    "        \n",
    "        #Select a random outcome, based on the unnormalised log-probabilities produced by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #Expand dimensions of prediction and assign as next input evaluation\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        #Convert prediction to char and append to generated list of text\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        \n",
    "    #return the initial input, concatenated with the generated text. \n",
    "    return(start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a testimony of the plainer\n",
      "    of such a paragon. I shall see him and be the face of a\n",
      "    speech, and the players of their death shall lose thee. Go, fellow,\n",
      "    and therefore am not so long as thou art. Do you not\n",
      "    think there was not the rest of thee to knock'd me to\n",
      "    his hands? Is he underneath the fall and cheerful rock?\n",
      "    The deadly stockings of the Spite of Troy,\n",
      "    With all the last of them and the adventure of the state,\n",
      "    That should be sure to see me light and look\n",
      "    To think t\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample text\n",
    "print(generate_text(model=test_model,start_seed=\"This is a test\", num_generate=500, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
